{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indonesian-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from django.core.validators import URLValidator\n",
    "from django.core.exceptions import ValidationError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "possible-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# where is data stored?\n",
    "data_dir = '../data/'\n",
    "\n",
    "# intermediates\n",
    "tribune_file = os.path.join(data_dir, 'tribune.tsv')\n",
    "sinclair_file = os.path.join(data_dir, 'sinclair.tsv')\n",
    "nexstar_file = os.path.join(data_dir, 'nexstar.tsv')\n",
    "meredith_file = os.path.join(data_dir, 'meredith.tsv')\n",
    "hearst_file = os.path.join(data_dir, 'hearst.tsv')\n",
    "stationindex_file = os.path.join(data_dir, 'station_index.tsv')\n",
    "usnpl_file = os.path.join(data_dir, 'usnpl.tsv')\n",
    "\n",
    "# this is where user entries go!\n",
    "custom_station_file = os.path.join(data_dir, 'custom_additions.json')\n",
    "\n",
    "# this is the output!\n",
    "local_news_dataset_file  = os.path.join(data_dir, 'local_news_dataset_2018.csv') \n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "# variables\n",
    "today = datetime.datetime.now()\n",
    "version = 0\n",
    "\n",
    "\n",
    "\n",
    "# for normalizing station info.\n",
    "owner_mapping = {\n",
    "    'Meredith Corporation' : 'Meredith',\n",
    "    'Sinclair Broadcast Group' : 'Sinclair',\n",
    "    'Nexstar Media Group' : 'Nexstar',\n",
    "    'Hearst Television' : 'Hearst'\n",
    "}\n",
    "\n",
    "station_index_mapping = {\n",
    "    'owner' : 'broadcaster'\n",
    "}\n",
    "\n",
    "national = [\n",
    "    'comettv.com',\n",
    "    'tbn.org',\n",
    "    'iontelevision.com',\n",
    "    'tct-net.org',\n",
    "    'sbgi.net',\n",
    "    'daystar.com',\n",
    "]\n",
    "\n",
    "look_up = {' Honolulu' : 'HI',\n",
    " ' Kalamazoo. MI' : 'MI',\n",
    "' San Antonio' : 'TX'}\n",
    "\n",
    "col_standard = {\n",
    "    'station' : 'name',\n",
    "    'twitter_name' : 'twitter',\n",
    "    'geography' : 'state',\n",
    "    'broadcaster' : 'owner'\n",
    "}\n",
    "\n",
    "cols_standard_nexstar = {\n",
    "    'Web Site' : 'website',\n",
    "    'Station' : 'station',\n",
    "    'Affiliation' : 'network'\n",
    "} \n",
    "\n",
    "cols_nexstar = ['station', 'website', 'city', 'state', 'broadcaster', 'source']\n",
    "\n",
    "cols = ['name', 'state', 'website', 'twitter', 'youtube', 'facebook', 'owner', 'medium', 'source', 'collection_date']\n",
    "cols_final = ['name', 'state', 'website', 'domain', 'twitter', 'youtube', 'facebook', 'owner', 'medium', 'source', 'collection_date']\n",
    "\n",
    "# to align nexstar websites to station names\n",
    "nexstar_alignment = {\n",
    "\n",
    "    'krqe.com' : [\n",
    "        'KRQE',\n",
    "        'KBIM',\n",
    "        'KREZ',\n",
    "    ],\n",
    "\n",
    "    'kwbq.com' : [\n",
    "        'KWBQ',\n",
    "        'KASY',\n",
    "        'KRWB'\n",
    "    ] ,\n",
    "\n",
    "    'kark.com' : [\n",
    "        'KARK',\n",
    "        'KARZ'\n",
    "    ],\n",
    "\n",
    "    'fox16.com' : [\n",
    "        'KLRT'\n",
    "    ],\n",
    "\n",
    "    'cwarkansas.com' : [\n",
    "        'KASN '\n",
    "    ],\n",
    "\n",
    "    'woodtv.com' : [\n",
    "        'WOOD',\n",
    "    ],\n",
    "\n",
    "    'wotv4women.com' : [\n",
    "        'WOTV',\n",
    "        'WXSP-CD'\n",
    "\n",
    "    ],\n",
    "    \n",
    "    'wkbn.com' : [\n",
    "        'WKBN'\n",
    "    ],\n",
    "    \n",
    "    'wytv.com' : [\n",
    "        'WYTV',\n",
    "        'WYFX-LD'\n",
    "    ]  \n",
    "}\n",
    "\n",
    "# for USNPL\n",
    "states = '''ak\t  al\t  ar\t  az\t  ca\t  co\t  ct\t  dc\t  de\t  fl\t  ga\t  hi\t  ia\t  id\t  il\t  in\t  ks   ky\t  la\t  ma\t  md\t  me\t  mi\t  mn\t  mo\t  ms\t  mt\t  nc\t  nd\t  ne\t  nh\t  nj\t  nm\t  nv\t  ny\t  oh\t  ok\t  or\t  pa\t  ri\t  sc\t  sd\t  tn\t  tx\t  ut\t  va\t  vt\t  wa\t  wi\t  wv\t  wy\t'''\n",
    "states = [s.strip() for s in states.split('  ')]\n",
    "\n",
    "# for stationindex\n",
    "city_state = {\n",
    "    'New York' : 'NY',\n",
    "    'Los Angeles' : 'CA',\n",
    "    'Chicago' : 'IL',\n",
    "    'Philadelphia' : 'PA',\n",
    "    'Dallas' : 'TX',\n",
    "    'Washington, D.C.' : 'DC',\n",
    "    'Houston' : \"TX\",\n",
    "    'Seattle' : 'WA',\n",
    "    'South Florida' : 'FL',\n",
    "    'Denver' : 'CO',\n",
    "    'Cleveland': 'OH',\n",
    "    'Sacramento' : 'CA',\n",
    "    'San Diego' : 'CA',\n",
    "    'St. Louis' : 'MO',\n",
    "    'Portland' : 'OR',\n",
    "    'Indianapolis' : 'IN',\n",
    "    'Hartford' :'CT',\n",
    "    'Kansas City' :'MO',\n",
    "    'Salt Lake City' : 'UT',\n",
    "    'Milwaukee' : 'WI',\n",
    "    'Waterbury' : 'CT',\n",
    "    'Grand Rapids' : 'MI',\n",
    "    'Oklahoma City': 'OK',\n",
    "    'Harrisburg' : 'VA',\n",
    "    'Norfolk' : 'VA',\n",
    "    'Greensboro/High Point/Winston-Salem' : 'NC',\n",
    "    'Memphis' : 'TN',\n",
    "    'New Orleans' : 'LA',\n",
    "    'Wilkes-Barre/Scranton' : 'PA',\n",
    "    'Richmond' : 'VA',\n",
    "    'Des Moines' : 'IL',\n",
    "    'Huntsville' : 'AL',\n",
    "    'Moline, IL / Davenport, IA' : \"IL/IA\",\n",
    "    'Fort Smith' : \"AK\",\n",
    "    'America' : 'National'\n",
    "}\n",
    "\n",
    "not_actually_local = [\n",
    "    'variety.com', 'investors.com', 'hollywoodreporter.com', 'bizjournals.com'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "geographic-classics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/3m5wyxtj3rbdf2hb3jxnf3gh0000gn/T/ipykernel_84966/3376430857.py:51: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  editor = sub_soup.find('strong', text='Editor:').find_next_sibling(text=True).strip()\n",
      "/var/folders/9l/3m5wyxtj3rbdf2hb3jxnf3gh0000gn/T/ipykernel_84966/3376430857.py:52: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  phone = sub_soup.find('strong', text='Phone:').find_next_sibling(text=True).strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bay Minette, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Columbiana, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Fayette, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Gulf Shores, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Lanett, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Opp, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Talladega, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Clovis, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Las Vegas, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Socorro, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "    State                       City                     Name  \\\n",
      "0      AL            Albertville, AL   Sand Mountain Reporter   \n",
      "1      AL         Alexander City, AL   Alexander City Outlook   \n",
      "2      AL              Andalusia, AL      Andalusia Star-News   \n",
      "3      AL               Anniston, AL            Anniston Star   \n",
      "4      AL                   Arab, AL             Arab Tribune   \n",
      "..    ...                        ...                      ...   \n",
      "131    NM                Socorro, NM    El Defensor Chieftain   \n",
      "132    NM               Timberon, NM  Timberon Mountain Times   \n",
      "133    NM  Truth or Consequences, NM                   Herald   \n",
      "134    NM  Truth or Consequences, NM   Sierra County Sentinel   \n",
      "135    NM              Tucumcari, NM          Quay County Sun   \n",
      "\n",
      "                                 Website                              Twitter  \\\n",
      "0    http://www.sandmountainreporter.com          https://twitter.com/smrnews   \n",
      "1         http://www.alexcityoutlook.com  https://twitter.com/alexcityoutlook   \n",
      "2       http://www.andalusiastarnews.com                                        \n",
      "3            http://www.annistonstar.com     https://twitter.com/annistonstar   \n",
      "4          http://www.thearabtribune.com      https://twitter.com/ArabTribune   \n",
      "..                                   ...                                  ...   \n",
      "131            http://www.dchieftain.com                                        \n",
      "132         http://www.mountaintimes.net                                        \n",
      "133             http://www.heraldpub.com                                        \n",
      "134                 https://gpkmedia.com                                        \n",
      "135           http://www.qcsunonline.com                                        \n",
      "\n",
      "                                              Facebook  \\\n",
      "0    https://www.facebook.com/pages/The-Sand-Mounta...   \n",
      "1    https://www.facebook.com/pages/The-Alexander-C...   \n",
      "2    https://www.facebook.com/pages/Andalusia-Star-...   \n",
      "3                https://www.facebook.com/annistonstar   \n",
      "4    https://www.facebook.com/pages/The-Arab-Tribun...   \n",
      "..                                                 ...   \n",
      "131       https://www.facebook.com/eldefensorchieftain   \n",
      "132                                                      \n",
      "133               https://www.facebook.com/TheHeraldNM   \n",
      "134  https://www.facebook.com/SierraCountySentinel....   \n",
      "135             https://www.facebook.com/QuayCountySun   \n",
      "\n",
      "                                   Instagram  \\\n",
      "0                                              \n",
      "1                                              \n",
      "2                                              \n",
      "3    https://www.instagram.com/annistonstar/   \n",
      "4                                              \n",
      "..                                       ...   \n",
      "131                                            \n",
      "132                                            \n",
      "133                                            \n",
      "134                                            \n",
      "135                                            \n",
      "\n",
      "                                               Youtube  \\\n",
      "0                                                        \n",
      "1     http://www.youtube.com/user/AlexanderCityOutlook   \n",
      "2                                                        \n",
      "3             http://www.youtube.com/user/AnnistonStar   \n",
      "4                                                        \n",
      "..                                                 ...   \n",
      "131  https://www.youtube.com/channel/UCoYiLP9wH8Hz7...   \n",
      "132                                                      \n",
      "133                                                      \n",
      "134                                                      \n",
      "135  https://www.youtube.com/channel/UC-XWuLgRCXgsx...   \n",
      "\n",
      "                                               Address             Editor  \\\n",
      "0          1603 Progress Dr Albertville, AL 35950-8547   Jonathan Stinson   \n",
      "1        548 Cherokee Rd Alexander City, AL 35010-2503        Mitch Sneed   \n",
      "2               207 Dunson St Andalusia, AL 36420-3705    Michele Gerlach   \n",
      "3                   PO Box 189 Anniston, AL 36202-0189          Bob Davis   \n",
      "4                       PO Box 605 Arab, AL 35016-0605  Charles Whisenant   \n",
      "..                                                 ...                ...   \n",
      "131              200 Winkler St Socorro, NM 87801-4200       Scott Turner   \n",
      "132                 PO Box 235 Timberon, NM 88350-0235    Darrell J. Pehr   \n",
      "133    PO Box 752 Truth or Consequences, NM 87901-0752        Mike Tooley   \n",
      "134  1747 E 3rd Ave Truth or Consequences, NM 87901...       Frances Luna   \n",
      "135               PO Box 1408 Tucumcari, NM 88401-1408      David Stevens   \n",
      "\n",
      "            Phone     source            collection_date  \n",
      "0    256-840-3000  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "1    256-234-4281  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "2    334-222-2402  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "3    256-236-1551  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "4    256-586-3188  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "..            ...        ...                        ...  \n",
      "131  575-835-0520  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "132  575-682-2208  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "133  575-894-2143  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "134  575-894-3088  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "135  575-461-1952  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "\n",
      "[136 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Edited USNPL Function\n",
    "\n",
    "def download_usnpl():\n",
    "    '''\n",
    "    usnpl has metadata about many newspapers in different states.\n",
    "    '''\n",
    "    \n",
    "    sites = []\n",
    "    \n",
    "#    for state in states:\n",
    "    for state in states:\n",
    "        url = 'https://www.usnpl.com/search/state?state={}'.format(state)\n",
    "        r = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "        \n",
    "        main_table = soup.find('table', class_='table table-sm')\n",
    "        \n",
    "        if main_table:\n",
    "            rows = main_table.find_all('tr')\n",
    "            # Remove non-data rows\n",
    "            rows = [row for row in rows if 'table-dark' not in row.get('class', [])]\n",
    "            current_city = \"\"\n",
    "            for row in rows:\n",
    "                city_element = row.find('h4', class_='result_city')\n",
    "                if city_element:\n",
    "                    current_city = city_element.text.strip()\n",
    "                    continue\n",
    "                # Extract data From the row\n",
    "                data_points = row.find_all('td')\n",
    "                if len(data_points) >= 6:\n",
    "                    newspaper_name = data_points[0].find('a').text.strip() if data_points[0].find('a') else ''\n",
    "                    usnpl_page = data_points[0].find('a')['href'] if data_points[0].find('a') else ''\n",
    "                    website = data_points[1].find('a')['href'] if data_points[1].find('a') else ''\n",
    "                    twitter = data_points[2].find('a')['href'] if data_points[2].find('a') else ''\n",
    "                    facebook = data_points[3].find('a')['href'] if data_points[3].find('a') else ''\n",
    "                    instagram = data_points[4].find('a')['href'] if data_points[4].find('a') else ''\n",
    "                    youtube = data_points[5].find('a')['href'] if data_points[5].find('a') else ''\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Extract Data From the Newspaper Page\n",
    "                sub_url = f\"https://www.usnpl.com/search/{usnpl_page}\"\n",
    "                r = requests.get(sub_url, headers=headers)\n",
    "                sub_soup = BeautifulSoup(r.content, 'lxml')\n",
    "                sub_table = sub_soup.find_all('tr')\n",
    "                address_element = sub_table[1]\n",
    "                address_parts = [part.strip() for part in address_element.stripped_strings]\n",
    "                address = ' '.join(address_parts)\n",
    "                editor = sub_soup.find('strong', text='Editor:').find_next_sibling(text=True).strip()\n",
    "                phone = sub_soup.find('strong', text='Phone:').find_next_sibling(text=True).strip()\n",
    "\n",
    "                # Parsed Object\n",
    "                parsed_object = {\n",
    "                    \"State\": state,\n",
    "                    \"City\": current_city,\n",
    "                    \"Name\": newspaper_name,\n",
    "                    \"Website\": website,\n",
    "                    \"Twitter\": twitter,\n",
    "                    \"Facebook\": facebook,\n",
    "                    \"Instagram\": instagram,\n",
    "                    \"Youtube\": youtube,\n",
    "                    \"Address\": address,\n",
    "                    \"Editor\": editor,\n",
    "                    \"Phone\": phone\n",
    "                }\n",
    "                \n",
    "                # Add to the list\n",
    "                sites.append(parsed_object)\n",
    "\n",
    "    df = pd.DataFrame(sites)\n",
    "    df['Website'] = df['Website'].str.rstrip('/')\n",
    "    df['source'] = 'usnpl.com'\n",
    "    df['collection_date'] = today\n",
    "    \n",
    "    if os.path.exists(usnpl_file):\n",
    "        # appending to old\n",
    "        df_ = pd.read_csv(usnpl_file, sep='\\t')\n",
    "        df = df[~df['Name'].isin(df_['Name'])]\n",
    "        df = df_.append(df) \n",
    "    \n",
    "    df.to_csv(usnpl_file, index=False, sep='\\t')\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "# download_usnpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "weekly-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Hearst\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['380 Main Street', 'Beaumont, TX 77701', '(409) 833-3311']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['117 North Second Street', 'Edwardsville, IL 62025', '(618) 656-4700']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['4747 Southwest Freeway', 'Houston, TX 77027', '(713) 220-7171']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['211 North Heisterman Street', 'Bad Axe, MI 48413', '(989) 269-6461']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['235 W. State Street', 'Jacksonville, IL 62650', '(217) 245-6121']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['111 Esperanza Drive', 'Laredo, TX 78041', '(956) 728-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['75 Maple Street', 'Manistee, MI 49660', '(231) 723-3592']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['219 East Main Street', 'Midland, MI 48640', '(989) 835-7171']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['201 E. Illinois', 'Midland, TX 79701', '(432) 682-5311']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['345 Main Street', 'Danbury, CT 06810', '(203) 744-5100']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06855', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['115 N. Michigan Avenue', 'Big Rapids, MI 49307', '(231) 796-4831']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['820 Broadway', 'Plainview, TX 79072', '(806) 296-1300']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['420 Broadway, Suite 200', 'San Antonio, TX 78205', '(210) 250-3000']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['901 Mission Street', 'San Francisco, CA 94103', '(415) 777-1111']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['701 5th Avenue', '48th Floor', 'Seattle, WA 98104', '(206) 448-8000']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['901 Mission Street', 'San Francisco, California 94103']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['301 Merritt 7, Suite 1', 'Norwalk, CT 06851', '(203) 842-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['219 Piasa Street', 'Alton, IL 62002', '(618) 463-2500']\n",
      "\n",
      "---address_list---\n",
      "\n",
      "['P.O. Box 15000', 'News Plaza', 'Albany, NY 12212', '(518) 454-5694']\n",
      "                                      website            station  \\\n",
      "0   https://www.facebook.com/centralcoastabc/  Central Coast ABC   \n",
      "1                        http://www.kcci.com/  Central Coast ABC   \n",
      "2                        http://www.kcra.com/  Central Coast ABC   \n",
      "3                  http://www.kmbc.com/kcwetv  Central Coast ABC   \n",
      "4                        http://www.ketv.com/  Central Coast ABC   \n",
      "..                                        ...                ...   \n",
      "20                  http://www.seattlepi.com/                      \n",
      "21                    https://www.sfgate.com/                      \n",
      "22          https://www.stamfordadvocate.com/                      \n",
      "23                   http://thetelegraph.com/                      \n",
      "24                https://www.timesunion.com/                      \n",
      "\n",
      "                  name                            phone  \\\n",
      "0    Central Coast ABC                                    \n",
      "1    Central Coast ABC                                    \n",
      "2    Central Coast ABC                                    \n",
      "3    Central Coast ABC                                    \n",
      "4    Central Coast ABC                                    \n",
      "..                 ...                              ...   \n",
      "20       Seattlepi.com                   (206) 448-8000   \n",
      "21              SFGATE  San Francisco, California 94103   \n",
      "22        The Advocate                   (203) 842-2500   \n",
      "23       The Telegraph                   (618) 463-2500   \n",
      "24  Albany Times Union                   (518) 454-5694   \n",
      "\n",
      "                                        address  \\\n",
      "0                                                 \n",
      "1                                                 \n",
      "2                                                 \n",
      "3                                                 \n",
      "4                                                 \n",
      "..                                          ...   \n",
      "20  701 5th Avenue 48th Floor Seattle, WA 98104   \n",
      "21                           901 Mission Street   \n",
      "22     301 Merritt 7, Suite 1 Norwalk, CT 06851   \n",
      "23             219 Piasa Street Alton, IL 62002   \n",
      "24   P.O. Box 15000 News Plaza Albany, NY 12212   \n",
      "\n",
      "                              twitter  \\\n",
      "0                                       \n",
      "1                                       \n",
      "2                                       \n",
      "3                                       \n",
      "4                                       \n",
      "..                                ...   \n",
      "20     https://twitter.com/seattlepi/   \n",
      "21        https://twitter.com/SFGate/   \n",
      "22  https://twitter.com/StamAdvocate/   \n",
      "23  http://twitter.com/altontelegraph   \n",
      "24    https://twitter.com/timesunion/   \n",
      "\n",
      "                                     facebook  \\\n",
      "0                                               \n",
      "1                                               \n",
      "2                                               \n",
      "3                                               \n",
      "4                                               \n",
      "..                                        ...   \n",
      "20   https://www.facebook.com/seattlepionline   \n",
      "21           https://www.facebook.com/SFGate/   \n",
      "22  https://www.facebook.com/stamfordadvocate   \n",
      "23          http://facebook.com/telegraphnews   \n",
      "24       https://www.facebook.com/timesunion/   \n",
      "\n",
      "                                        linkedin  \\\n",
      "0                                                  \n",
      "1                                                  \n",
      "2                                                  \n",
      "3                                                  \n",
      "4                                                  \n",
      "..                                           ...   \n",
      "20                                                 \n",
      "21       https://www.linkedin.com/company/sfgate   \n",
      "22                                                 \n",
      "23                                                 \n",
      "24  https://www.linkedin.com/company/times-union   \n",
      "\n",
      "                                      instagram broadcaster  \\\n",
      "0                                                    Hearst   \n",
      "1                                                    Hearst   \n",
      "2                                                    Hearst   \n",
      "3                                                    Hearst   \n",
      "4                                                    Hearst   \n",
      "..                                          ...         ...   \n",
      "20         https://www.instagram.com/seattlepi/      Hearst   \n",
      "21            https://www.instagram.com/SFGate/      Hearst   \n",
      "22  https://www.instagram.com/stamfordadvocate/      Hearst   \n",
      "23                                                   Hearst   \n",
      "24        https://www.instagram.com/timesunion/      Hearst   \n",
      "\n",
      "                     source            collection_date  \n",
      "0   https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "1   https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "2   https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "3   https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "4   https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "..                      ...                        ...  \n",
      "20  https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "21  https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "22  https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "23  https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "24  https://www.hearst.com/ 2023-05-17 08:37:18.925904  \n",
      "\n",
      "[62 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "def download_hearst():\n",
    "    '''\n",
    "    Downloads metadata about Hearst newspapers and broadcasting channels.\n",
    "\n",
    "    The final DataFrame includes details such as website, name, address, phone, Twitter, Facebook, LinkedIn, \n",
    "    Instagram, station name (for broadcasting channels), broadcaster (set as \"Hearst\"), source (set as \n",
    "    \"https://www.hearst.com/\"), and collection date.\n",
    "\n",
    "    Note: The function requires the requests, BeautifulSoup, and pandas libraries.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    \n",
    "    # Parse the broadcasting channels\n",
    "    def parse_channel_html(channel_html):\n",
    "        '''Parses bs4 html to create a dictionary (row in the dataset)'''\n",
    "        website_tag = channel_html.find('a')\n",
    "\n",
    "        # Sometime there are brand-cards that don't have any metadata attached\n",
    "        if website_tag is not None:\n",
    "            website = website_tag.get('href')\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Extract station name from alt-text\n",
    "        img_container = soup.find('div', class_='brand-logo-caption-with-text')\n",
    "        image_element = img_container.find('img')\n",
    "        station = alt_text = image_element['alt']\n",
    "\n",
    "        context = dict(\n",
    "            website = website,\n",
    "            station = station,\n",
    "            name = station,\n",
    "            phone = \"\",\n",
    "            address = \"\",\n",
    "            twitter = \"\",\n",
    "            facebook = \"\",\n",
    "            linkedin = \"\",\n",
    "            instagram = \"\"\n",
    "        )\n",
    "\n",
    "        return context\n",
    "    \n",
    "    \n",
    "    # Parse the newspaper pages\n",
    "    def parse_newspaper_html(newspaper_html):        \n",
    "        '''Parses bs4 html to create a dictionary (row in the dataset)'''\n",
    "        \n",
    "        href = newspaper_html.find('a').get('href')\n",
    "        sub_r = requests.get(f'https://www.hearst.com{href}', headers=headers)\n",
    "        sub_soup = BeautifulSoup(sub_r.content, 'lxml')\n",
    "        \n",
    "        # Extract newspaper information\n",
    "        data_section = sub_soup.find('section', id='content')\n",
    "        name = data_section.find(\"h1\").text.strip()\n",
    "        \n",
    "        main_column = data_section.find('div', id='layout-column_column-1')\n",
    "        column_divs = main_column.find_all('div', recursive=False)\n",
    "\n",
    "        contact_info = column_divs[2].find('div', class_=\"brand-contact-info\")\n",
    "        website = contact_info.find('p', class_=\"brand-address\").find('a').get('href')\n",
    "        \n",
    "        address_info = column_divs[2].find('div', class_='address-container')\n",
    "        address_list = [p.text.strip() for p in address_info.find_all('p')]\n",
    "        phone = address_list[-1]\n",
    "        address = ' '.join(address_list[:-1])\n",
    "            \n",
    "        social_info = column_divs[2].find('ul', class_=\"brand-icons\")\n",
    "        twitter = ''\n",
    "        facebook = ''\n",
    "        linkedin = ''\n",
    "        instagram = ''\n",
    "        for link in social_info.find_all('a'):\n",
    "            img_alt = link.find('img')['alt']\n",
    "            href = link['href']\n",
    "            if 'twitter' in img_alt.lower():\n",
    "                twitter = href\n",
    "            elif 'facebook' in img_alt.lower():\n",
    "                facebook = href\n",
    "            elif 'linkedin' in img_alt.lower():\n",
    "                linkedin = href\n",
    "            elif 'instagram' in img_alt.lower():\n",
    "                instagram = href\n",
    "        \n",
    "        column_divs[2]\n",
    "\n",
    "        context = dict(\n",
    "            website = website,\n",
    "            name = name,\n",
    "            address = address,\n",
    "            phone = phone,\n",
    "            twitter = twitter,\n",
    "            facebook = facebook,\n",
    "            linkedin = linkedin,\n",
    "            instagram = instagram,\n",
    "            station = \"\"\n",
    "        )\n",
    "\n",
    "        return context\n",
    "    \n",
    "    # -- -- -- -- -- -- -- -- -- -- -- -- --\n",
    "    \n",
    "    print(\"Downloading Hearst\")\n",
    "    broadcasting_url = \"https://www.hearst.com/broadcasting\"\n",
    "    newspaper_url = \"https://www.hearst.com/newspapers\"\n",
    "    \n",
    "    # Get broadcasting data\n",
    "    r = requests.get(broadcasting_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    parent_div = soup.find('div', class_='brand-card')\n",
    "    channels = parent_div.find_all('div', recursive=False)\n",
    "    channel_metadata = []\n",
    "    for channel in channels:\n",
    "        channel_meta = parse_channel_html(channel)\n",
    "        if channel_meta is not None:\n",
    "            channel_metadata.append(channel_meta)\n",
    "    \n",
    "    # get newspaper data\n",
    "    r = requests.get(newspaper_url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    parent_div = soup.find('div', class_='brand-card')\n",
    "    newspapers = parent_div.find_all('div', recursive=False)\n",
    "    newspaper_metadata = []\n",
    "    for newspaper in newspapers:\n",
    "        newspaper_meta = parse_newspaper_html(newspaper)\n",
    "        newspaper_metadata.append(newspaper_meta)  \n",
    "    \n",
    "    broadcast_df = pd.DataFrame(channel_metadata)\n",
    "    newspaper_df = pd.DataFrame(newspaper_metadata)\n",
    "    \n",
    "    df = pd.concat([broadcast_df, newspaper_df])\n",
    "    \n",
    "    df['broadcaster'] = 'Hearst'\n",
    "    df['source'] = 'https://www.hearst.com/'\n",
    "    df['collection_date'] = today\n",
    "    \n",
    "    if os.path.exists(hearst_file):\n",
    "        # appending to old\n",
    "        df_ = pd.read_csv(hearst_file, sep='\\t')\n",
    "        df = df[~df['station'].isin(df_['station'])]\n",
    "        df = df_.append(df) \n",
    "    \n",
    "    df.to_csv(hearst_file, index=False, sep='\\t')\n",
    "    \n",
    "download_hearst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-composer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
