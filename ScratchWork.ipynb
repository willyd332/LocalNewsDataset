{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indonesian-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from django.core.validators import URLValidator\n",
    "from django.core.exceptions import ValidationError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "possible-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# where is data stored?\n",
    "data_dir = '../data/'\n",
    "\n",
    "# intermediates\n",
    "tribune_file = os.path.join(data_dir, 'tribune.tsv')\n",
    "sinclair_file = os.path.join(data_dir, 'sinclair.tsv')\n",
    "nexstar_file = os.path.join(data_dir, 'nexstar.tsv')\n",
    "meredith_file = os.path.join(data_dir, 'meredith.tsv')\n",
    "hearst_file = os.path.join(data_dir, 'hearst.tsv')\n",
    "stationindex_file = os.path.join(data_dir, 'station_index.tsv')\n",
    "usnpl_file = os.path.join(data_dir, 'usnpl.tsv')\n",
    "\n",
    "# this is where user entries go!\n",
    "custom_station_file = os.path.join(data_dir, 'custom_additions.json')\n",
    "\n",
    "# this is the output!\n",
    "local_news_dataset_file  = os.path.join(data_dir, 'local_news_dataset_2018.csv') \n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
    "\n",
    "# variables\n",
    "today = datetime.datetime.now()\n",
    "version = 0\n",
    "\n",
    "\n",
    "\n",
    "# for normalizing station info.\n",
    "owner_mapping = {\n",
    "    'Meredith Corporation' : 'Meredith',\n",
    "    'Sinclair Broadcast Group' : 'Sinclair',\n",
    "    'Nexstar Media Group' : 'Nexstar',\n",
    "    'Hearst Television' : 'Hearst'\n",
    "}\n",
    "\n",
    "station_index_mapping = {\n",
    "    'owner' : 'broadcaster'\n",
    "}\n",
    "\n",
    "national = [\n",
    "    'comettv.com',\n",
    "    'tbn.org',\n",
    "    'iontelevision.com',\n",
    "    'tct-net.org',\n",
    "    'sbgi.net',\n",
    "    'daystar.com',\n",
    "]\n",
    "\n",
    "look_up = {' Honolulu' : 'HI',\n",
    " ' Kalamazoo. MI' : 'MI',\n",
    "' San Antonio' : 'TX'}\n",
    "\n",
    "col_standard = {\n",
    "    'station' : 'name',\n",
    "    'twitter_name' : 'twitter',\n",
    "    'geography' : 'state',\n",
    "    'broadcaster' : 'owner'\n",
    "}\n",
    "\n",
    "cols_standard_nexstar = {\n",
    "    'Web Site' : 'website',\n",
    "    'Station' : 'station',\n",
    "    'Affiliation' : 'network'\n",
    "} \n",
    "\n",
    "cols_nexstar = ['station', 'website', 'city', 'state', 'broadcaster', 'source']\n",
    "\n",
    "cols = ['name', 'state', 'website', 'twitter', 'youtube', 'facebook', 'owner', 'medium', 'source', 'collection_date']\n",
    "cols_final = ['name', 'state', 'website', 'domain', 'twitter', 'youtube', 'facebook', 'owner', 'medium', 'source', 'collection_date']\n",
    "\n",
    "# to align nexstar websites to station names\n",
    "nexstar_alignment = {\n",
    "\n",
    "    'krqe.com' : [\n",
    "        'KRQE',\n",
    "        'KBIM',\n",
    "        'KREZ',\n",
    "    ],\n",
    "\n",
    "    'kwbq.com' : [\n",
    "        'KWBQ',\n",
    "        'KASY',\n",
    "        'KRWB'\n",
    "    ] ,\n",
    "\n",
    "    'kark.com' : [\n",
    "        'KARK',\n",
    "        'KARZ'\n",
    "    ],\n",
    "\n",
    "    'fox16.com' : [\n",
    "        'KLRT'\n",
    "    ],\n",
    "\n",
    "    'cwarkansas.com' : [\n",
    "        'KASN '\n",
    "    ],\n",
    "\n",
    "    'woodtv.com' : [\n",
    "        'WOOD',\n",
    "    ],\n",
    "\n",
    "    'wotv4women.com' : [\n",
    "        'WOTV',\n",
    "        'WXSP-CD'\n",
    "\n",
    "    ],\n",
    "    \n",
    "    'wkbn.com' : [\n",
    "        'WKBN'\n",
    "    ],\n",
    "    \n",
    "    'wytv.com' : [\n",
    "        'WYTV',\n",
    "        'WYFX-LD'\n",
    "    ]  \n",
    "}\n",
    "\n",
    "# for USNPL\n",
    "states = '''ak\t  al\t  ar\t  az\t  ca\t  co\t  ct\t  dc\t  de\t  fl\t  ga\t  hi\t  ia\t  id\t  il\t  in\t  ks   ky\t  la\t  ma\t  md\t  me\t  mi\t  mn\t  mo\t  ms\t  mt\t  nc\t  nd\t  ne\t  nh\t  nj\t  nm\t  nv\t  ny\t  oh\t  ok\t  or\t  pa\t  ri\t  sc\t  sd\t  tn\t  tx\t  ut\t  va\t  vt\t  wa\t  wi\t  wv\t  wy\t'''\n",
    "states = [s.strip() for s in states.split('  ')]\n",
    "\n",
    "# for stationindex\n",
    "city_state = {\n",
    "    'New York' : 'NY',\n",
    "    'Los Angeles' : 'CA',\n",
    "    'Chicago' : 'IL',\n",
    "    'Philadelphia' : 'PA',\n",
    "    'Dallas' : 'TX',\n",
    "    'Washington, D.C.' : 'DC',\n",
    "    'Houston' : \"TX\",\n",
    "    'Seattle' : 'WA',\n",
    "    'South Florida' : 'FL',\n",
    "    'Denver' : 'CO',\n",
    "    'Cleveland': 'OH',\n",
    "    'Sacramento' : 'CA',\n",
    "    'San Diego' : 'CA',\n",
    "    'St. Louis' : 'MO',\n",
    "    'Portland' : 'OR',\n",
    "    'Indianapolis' : 'IN',\n",
    "    'Hartford' :'CT',\n",
    "    'Kansas City' :'MO',\n",
    "    'Salt Lake City' : 'UT',\n",
    "    'Milwaukee' : 'WI',\n",
    "    'Waterbury' : 'CT',\n",
    "    'Grand Rapids' : 'MI',\n",
    "    'Oklahoma City': 'OK',\n",
    "    'Harrisburg' : 'VA',\n",
    "    'Norfolk' : 'VA',\n",
    "    'Greensboro/High Point/Winston-Salem' : 'NC',\n",
    "    'Memphis' : 'TN',\n",
    "    'New Orleans' : 'LA',\n",
    "    'Wilkes-Barre/Scranton' : 'PA',\n",
    "    'Richmond' : 'VA',\n",
    "    'Des Moines' : 'IL',\n",
    "    'Huntsville' : 'AL',\n",
    "    'Moline, IL / Davenport, IA' : \"IL/IA\",\n",
    "    'Fort Smith' : \"AK\",\n",
    "    'America' : 'National'\n",
    "}\n",
    "\n",
    "not_actually_local = [\n",
    "    'variety.com', 'investors.com', 'hollywoodreporter.com', 'bizjournals.com'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "geographic-classics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/3m5wyxtj3rbdf2hb3jxnf3gh0000gn/T/ipykernel_84966/3376430857.py:51: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  editor = sub_soup.find('strong', text='Editor:').find_next_sibling(text=True).strip()\n",
      "/var/folders/9l/3m5wyxtj3rbdf2hb3jxnf3gh0000gn/T/ipykernel_84966/3376430857.py:52: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  phone = sub_soup.find('strong', text='Phone:').find_next_sibling(text=True).strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bay Minette, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Columbiana, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Fayette, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Gulf Shores, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Lanett, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Opp, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Talladega, AL\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Clovis, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Las Vegas, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "Socorro, NM\n",
      "[<td colspan=\"6\">\n",
      "</td>]\n",
      "    State                       City                     Name  \\\n",
      "0      AL            Albertville, AL   Sand Mountain Reporter   \n",
      "1      AL         Alexander City, AL   Alexander City Outlook   \n",
      "2      AL              Andalusia, AL      Andalusia Star-News   \n",
      "3      AL               Anniston, AL            Anniston Star   \n",
      "4      AL                   Arab, AL             Arab Tribune   \n",
      "..    ...                        ...                      ...   \n",
      "131    NM                Socorro, NM    El Defensor Chieftain   \n",
      "132    NM               Timberon, NM  Timberon Mountain Times   \n",
      "133    NM  Truth or Consequences, NM                   Herald   \n",
      "134    NM  Truth or Consequences, NM   Sierra County Sentinel   \n",
      "135    NM              Tucumcari, NM          Quay County Sun   \n",
      "\n",
      "                                 Website                              Twitter  \\\n",
      "0    http://www.sandmountainreporter.com          https://twitter.com/smrnews   \n",
      "1         http://www.alexcityoutlook.com  https://twitter.com/alexcityoutlook   \n",
      "2       http://www.andalusiastarnews.com                                        \n",
      "3            http://www.annistonstar.com     https://twitter.com/annistonstar   \n",
      "4          http://www.thearabtribune.com      https://twitter.com/ArabTribune   \n",
      "..                                   ...                                  ...   \n",
      "131            http://www.dchieftain.com                                        \n",
      "132         http://www.mountaintimes.net                                        \n",
      "133             http://www.heraldpub.com                                        \n",
      "134                 https://gpkmedia.com                                        \n",
      "135           http://www.qcsunonline.com                                        \n",
      "\n",
      "                                              Facebook  \\\n",
      "0    https://www.facebook.com/pages/The-Sand-Mounta...   \n",
      "1    https://www.facebook.com/pages/The-Alexander-C...   \n",
      "2    https://www.facebook.com/pages/Andalusia-Star-...   \n",
      "3                https://www.facebook.com/annistonstar   \n",
      "4    https://www.facebook.com/pages/The-Arab-Tribun...   \n",
      "..                                                 ...   \n",
      "131       https://www.facebook.com/eldefensorchieftain   \n",
      "132                                                      \n",
      "133               https://www.facebook.com/TheHeraldNM   \n",
      "134  https://www.facebook.com/SierraCountySentinel....   \n",
      "135             https://www.facebook.com/QuayCountySun   \n",
      "\n",
      "                                   Instagram  \\\n",
      "0                                              \n",
      "1                                              \n",
      "2                                              \n",
      "3    https://www.instagram.com/annistonstar/   \n",
      "4                                              \n",
      "..                                       ...   \n",
      "131                                            \n",
      "132                                            \n",
      "133                                            \n",
      "134                                            \n",
      "135                                            \n",
      "\n",
      "                                               Youtube  \\\n",
      "0                                                        \n",
      "1     http://www.youtube.com/user/AlexanderCityOutlook   \n",
      "2                                                        \n",
      "3             http://www.youtube.com/user/AnnistonStar   \n",
      "4                                                        \n",
      "..                                                 ...   \n",
      "131  https://www.youtube.com/channel/UCoYiLP9wH8Hz7...   \n",
      "132                                                      \n",
      "133                                                      \n",
      "134                                                      \n",
      "135  https://www.youtube.com/channel/UC-XWuLgRCXgsx...   \n",
      "\n",
      "                                               Address             Editor  \\\n",
      "0          1603 Progress Dr Albertville, AL 35950-8547   Jonathan Stinson   \n",
      "1        548 Cherokee Rd Alexander City, AL 35010-2503        Mitch Sneed   \n",
      "2               207 Dunson St Andalusia, AL 36420-3705    Michele Gerlach   \n",
      "3                   PO Box 189 Anniston, AL 36202-0189          Bob Davis   \n",
      "4                       PO Box 605 Arab, AL 35016-0605  Charles Whisenant   \n",
      "..                                                 ...                ...   \n",
      "131              200 Winkler St Socorro, NM 87801-4200       Scott Turner   \n",
      "132                 PO Box 235 Timberon, NM 88350-0235    Darrell J. Pehr   \n",
      "133    PO Box 752 Truth or Consequences, NM 87901-0752        Mike Tooley   \n",
      "134  1747 E 3rd Ave Truth or Consequences, NM 87901...       Frances Luna   \n",
      "135               PO Box 1408 Tucumcari, NM 88401-1408      David Stevens   \n",
      "\n",
      "            Phone     source            collection_date  \n",
      "0    256-840-3000  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "1    256-234-4281  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "2    334-222-2402  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "3    256-236-1551  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "4    256-586-3188  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "..            ...        ...                        ...  \n",
      "131  575-835-0520  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "132  575-682-2208  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "133  575-894-2143  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "134  575-894-3088  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "135  575-461-1952  usnpl.com 2023-05-16 22:28:44.866254  \n",
      "\n",
      "[136 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "test_states = [\"AL\", \"NM\"]\n",
    "\n",
    "def download_usnpl():\n",
    "    '''\n",
    "    usnpl has metadata about many newspapers in different states.\n",
    "    '''\n",
    "    \n",
    "    sites = []\n",
    "    \n",
    "#    for state in states:\n",
    "    for state in test_states:\n",
    "        url = 'https://www.usnpl.com/search/state?state={}'.format(state)\n",
    "        r = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(r.content, 'lxml')\n",
    "        \n",
    "        main_table = soup.find('table', class_='table table-sm')\n",
    "        \n",
    "        if main_table:\n",
    "            rows = main_table.find_all('tr')\n",
    "            # Remove non-data rows\n",
    "            rows = [row for row in rows if 'table-dark' not in row.get('class', [])]\n",
    "            current_city = \"\"\n",
    "            for row in rows:\n",
    "                city_element = row.find('h4', class_='result_city')\n",
    "                if city_element:\n",
    "                    current_city = city_element.text.strip()\n",
    "                    continue\n",
    "                # Extract data From the row\n",
    "                data_points = row.find_all('td')\n",
    "                if len(data_points) >= 6:\n",
    "                    newspaper_name = data_points[0].find('a').text.strip() if data_points[0].find('a') else ''\n",
    "                    usnpl_page = data_points[0].find('a')['href'] if data_points[0].find('a') else ''\n",
    "                    website = data_points[1].find('a')['href'] if data_points[1].find('a') else ''\n",
    "                    twitter = data_points[2].find('a')['href'] if data_points[2].find('a') else ''\n",
    "                    facebook = data_points[3].find('a')['href'] if data_points[3].find('a') else ''\n",
    "                    instagram = data_points[4].find('a')['href'] if data_points[4].find('a') else ''\n",
    "                    youtube = data_points[5].find('a')['href'] if data_points[5].find('a') else ''\n",
    "                else:\n",
    "                    print(current_city)\n",
    "                    print(data_points)\n",
    "                    continue\n",
    "\n",
    "                # Extract Data From the Newspaper Page\n",
    "                sub_url = f\"https://www.usnpl.com/search/{usnpl_page}\"\n",
    "                r = requests.get(sub_url, headers=headers)\n",
    "                sub_soup = BeautifulSoup(r.content, 'lxml')\n",
    "                sub_table = sub_soup.find_all('tr')\n",
    "                address_element = sub_table[1]\n",
    "                address_parts = [part.strip() for part in address_element.stripped_strings]\n",
    "                address = ' '.join(address_parts)\n",
    "                editor = sub_soup.find('strong', text='Editor:').find_next_sibling(text=True).strip()\n",
    "                phone = sub_soup.find('strong', text='Phone:').find_next_sibling(text=True).strip()\n",
    "\n",
    "                # Parsed Object\n",
    "                parsed_object = {\n",
    "                    \"State\": state,\n",
    "                    \"City\": current_city,\n",
    "                    \"Name\": newspaper_name,\n",
    "                    \"Website\": website,\n",
    "                    \"Twitter\": twitter,\n",
    "                    \"Facebook\": facebook,\n",
    "                    \"Instagram\": instagram,\n",
    "                    \"Youtube\": youtube,\n",
    "                    \"Address\": address,\n",
    "                    \"Editor\": editor,\n",
    "                    \"Phone\": phone\n",
    "                }\n",
    "                \n",
    "                # Add to the list\n",
    "                sites.append(parsed_object)\n",
    "\n",
    "    df = pd.DataFrame(sites)\n",
    "    df['Website'] = df['Website'].str.rstrip('/')\n",
    "    df['source'] = 'usnpl.com'\n",
    "    df['collection_date'] = today\n",
    "    \n",
    "#     if os.path.exists(usnpl_file):\n",
    "#         # appending to old\n",
    "#         df_ = pd.read_csv(usnpl_file, sep='\\t')\n",
    "#         df = df[~df['Name'].isin(df_['Name'])]\n",
    "#         df = df_.append(df) \n",
    "    \n",
    "#     df.to_csv(usnpl_file, index=False, sep='\\t')\n",
    "\n",
    "    print(df)\n",
    "    \n",
    "download_usnpl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
